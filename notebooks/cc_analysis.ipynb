{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d08c17",
   "metadata": {},
   "source": [
    "# Analysis for Credit Card Sales "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc209cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0718a",
   "metadata": {},
   "source": [
    "## Preprocessing steps"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ae035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Contents of current directory:\", os.listdir('.'))\n",
    "excel_path = os.path.join('..', 'data', 'raw', 'DataScientist_CaseStudy_Dataset.xlsx')\n",
    "xls = pd.ExcelFile(excel_path)\n",
    "datasets = {sheet: pd.read_excel(xls, sheet_name=sheet, engine=\"openpyxl\") for sheet in xls.sheet_names}\n",
    "print(\"All sheet names:\", list(datasets.keys()))\n",
    "print(\"Available datasets:\", datasets.keys())\n",
    "raw_datasets = datasets.pop('Description')\n",
    "# check the length of each dataset\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name}: {len(df)} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets.values():\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13804d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clients from Sales_Revenues dataset\n",
    "sales_clients = set(datasets['Sales_Revenues']['Client'].unique())\n",
    "print(f\"Clients in Sales_Revenues: {len(sales_clients)}\")\n",
    "\n",
    "# Create train and test datasets\n",
    "datasets_train = {}\n",
    "datasets_test = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if 'Client' in df.columns:\n",
    "        # Train: clients that exist in Sales_Revenues\n",
    "        train_mask = df['Client'].isin(sales_clients)\n",
    "        train_df = df[train_mask].copy()\n",
    "        \n",
    "        # Test: clients that do NOT exist in Sales_Revenues\n",
    "        test_df = df[~train_mask].copy()\n",
    "        \n",
    "        # Store with new names\n",
    "        datasets_train[f\"{name}_train\"] = train_df\n",
    "        datasets_test[f\"{name}_test\"] = test_df\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Total rows: {len(df)}\")\n",
    "        print(f\"  Train rows: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Test rows: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Train unique clients: {train_df['Client'].nunique()}\")\n",
    "        print(f\"  Test unique clients: {test_df['Client'].nunique()}\")\n",
    "        print()\n",
    "\n",
    "# Summary\n",
    "print(\"=== TRAIN DATASETS ===\")\n",
    "for name, df in datasets_train.items():\n",
    "    print(f\"{name}: {len(df)} rows, {df['Client'].nunique()} unique clients\")\n",
    "\n",
    "print(\"\\n=== TEST DATASETS ===\")\n",
    "for name, df in datasets_test.items():\n",
    "    print(f\"{name}: {len(df)} rows, {df['Client'].nunique()} unique clients\")\n",
    "\n",
    "# Verification\n",
    "print(f\"\\n=== VERIFICATION ===\")\n",
    "print(f\"Sales_Revenues clients: {len(sales_clients)}\")\n",
    "print(f\"Sales_Revenues_train clients: {datasets_train['Sales_Revenues_train']['Client'].nunique()}\")\n",
    "print(f\"Sales_Revenues_test clients: {datasets_test['Sales_Revenues_test']['Client'].nunique()}\")\n",
    "print(f\"Should be equal: {len(sales_clients) == datasets_train['Sales_Revenues_train']['Client'].nunique()}\")\n",
    "print(f\"Test should be 0: {datasets_test['Sales_Revenues_test']['Client'].nunique() == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed359755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only Revenue_CC and Sale_CC\n",
    "datasets_train['Sales_Revenues_train'] = datasets_train['Sales_Revenues_train'][['Client', 'Revenue_CC', 'Sale_CC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187636d1",
   "metadata": {},
   "source": [
    "## Data Leakage Factors:\n",
    "- `Count_CC` : number of live mutual funds\n",
    "- `ActBal_CC` : actual mutual funds balance [EUR]\n",
    "- `Revenue_CC` : target value\n",
    "# Medium risk leakage factors that may correlate too strongly:\n",
    "- `VolumeCred` : monthly credit turnover [EUR] - may reflect CC contributions\n",
    "- `VolumeDeb` : monthly debit turnover [EUR] - may reflect CC withdrawals  \n",
    "- `TransactionsCred` : number of all credit transactions - investment activity\n",
    "- `TransactionsDeb` : number of all debit transactions - investment activity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262603dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_training_data(datasets_dict, base_dataset_key='Sales_Revenues_train', \n",
    "                           datasets_to_merge=None, join_key='Client'):\n",
    "    \"\"\"\n",
    "    Merge all training datasets on a specified key for comprehensive analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets_dict : dict\n",
    "        Dictionary containing all datasets to merge\n",
    "    base_dataset_key : str\n",
    "        Key for the base dataset (default: 'Sales_Revenues_train')\n",
    "    datasets_to_merge : list, optional\n",
    "        List of dataset keys to merge. If None, merges all except base\n",
    "    join_key : str\n",
    "        Column name to join on (default: 'Client')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Merged dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MERGING ALL TRAINING DATASETS ===\")\n",
    "    \n",
    "    # Validate base dataset exists\n",
    "    if base_dataset_key not in datasets_dict:\n",
    "        raise ValueError(f\"Base dataset '{base_dataset_key}' not found in datasets\")\n",
    "    \n",
    "    # Start with base dataset\n",
    "    merged_df = datasets_dict[base_dataset_key].copy()\n",
    "    print(f\"Base dataset ({base_dataset_key}): {merged_df.shape}\")\n",
    "    \n",
    "    # Determine datasets to merge\n",
    "    if datasets_to_merge is None:\n",
    "        datasets_to_merge = [key for key in datasets_dict.keys() if key != base_dataset_key]\n",
    "    \n",
    "    # Validate join key exists in base dataset\n",
    "    if join_key not in merged_df.columns:\n",
    "        raise ValueError(f\"Join key '{join_key}' not found in base dataset\")\n",
    "    \n",
    "    # Merge each dataset\n",
    "    for dataset_name in datasets_to_merge:\n",
    "        if dataset_name in datasets_dict:\n",
    "            df_to_merge = datasets_dict[dataset_name]\n",
    "            print(f\"\\nMerging {dataset_name}: {df_to_merge.shape}\")\n",
    "            \n",
    "            # Check for join key column\n",
    "            if join_key in df_to_merge.columns:\n",
    "                # Merge on join key\n",
    "                before_shape = merged_df.shape\n",
    "                merged_df = merged_df.merge(df_to_merge, on=join_key, how='left', suffixes=('', '_dup'))\n",
    "                after_shape = merged_df.shape\n",
    "                \n",
    "                print(f\"  Before merge: {before_shape}\")\n",
    "                print(f\"  After merge: {after_shape}\")\n",
    "                print(f\"  Records with {join_key}: {merged_df[join_key].nunique()}\")\n",
    "                \n",
    "                # Check for duplicate columns and handle them\n",
    "                duplicate_cols = [col for col in merged_df.columns if col.endswith('_dup')]\n",
    "                if duplicate_cols:\n",
    "                    print(f\"  \u26a0\ufe0f  Duplicate columns found: {[col.replace('_dup', '') for col in duplicate_cols]}\")\n",
    "                    # Drop duplicate columns\n",
    "                    merged_df = merged_df.drop(columns=duplicate_cols)\n",
    "                    print(f\"  \u2705 Dropped duplicate columns\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  \u26a0\ufe0f  No '{join_key}' column found in {dataset_name}\")\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  {dataset_name} not found in datasets\")\n",
    "    \n",
    "    print(f\"\\n=== FINAL MERGED DATASET ===\")\n",
    "    print(f\"Shape: {merged_df.shape}\")\n",
    "    print(f\"Unique {join_key}s: {merged_df[join_key].nunique()}\")\n",
    "    print(f\"Columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def analyze_data_quality(df, show_columns=True, max_missing_display=20):\n",
    "    \"\"\"\n",
    "    Analyze data quality of a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset to analyze\n",
    "    show_columns : bool\n",
    "        Whether to display all column names\n",
    "    max_missing_display : int\n",
    "        Maximum number of missing value columns to display\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== DATA QUALITY CHECK ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Display column names if requested\n",
    "    if show_columns:\n",
    "        print(f\"\\nColumn names:\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values per column:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        print(f\"Found {len(missing_data)} columns with missing values:\")\n",
    "        display_missing = missing_data.head(max_missing_display)\n",
    "        for col, count in display_missing.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if len(missing_data) > max_missing_display:\n",
    "            print(f\"  ... and {len(missing_data) - max_missing_display} more columns\")\n",
    "    else:\n",
    "        print(\"No missing values found! \u2705\")\n",
    "\n",
    "    # Check data types\n",
    "    print(f\"\\nData types summary:\")\n",
    "    dtype_summary = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_summary.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    print(f\"\\nPotential issues:\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"  \u26a0\ufe0f  {duplicates} duplicate rows found\")\n",
    "    else:\n",
    "        print(f\"  \u2705 No duplicate rows\")\n",
    "    \n",
    "    # Check for constant columns\n",
    "    constant_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() <= 1:\n",
    "            constant_cols.append(col)\n",
    "    \n",
    "    if constant_cols:\n",
    "        print(f\"  \u26a0\ufe0f  Constant columns (nunique \u2264 1): {constant_cols}\")\n",
    "    else:\n",
    "        print(f\"  \u2705 No constant columns\")\n",
    "\n",
    "\n",
    "# Reusable function call with parameters\n",
    "combined_train_data = merge_all_training_data(\n",
    "    datasets_dict=datasets_train,\n",
    "    base_dataset_key='Sales_Revenues_train',\n",
    "    join_key='Client'\n",
    ")\n",
    "\n",
    "# Analyze data quality\n",
    "analyze_data_quality(combined_train_data, show_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848991f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get numeric columns only\n",
    "numeric_data = combined_train_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix using scipy\n",
    "corr_matrix = numeric_data.corr(method='pearson')\n",
    "\n",
    "# Create correlation plot\n",
    "plt.figure(figsize=(24, 20))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "\n",
    "plt.title('Correlation Matrix - Combined Training Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a754baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Sale_CC\n",
    "plt.Figure(figsize=(8, 6))\n",
    "sns.countplot(data=combined_train_data, x='Sale_CC')\n",
    "plt.title('Proportion of Sale_CC')\n",
    "plt.xlabel('Sale_CC')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Percentage\n",
    "sale_mf_counts = combined_train_data['Sale_CC'].value_counts(normalize=True) * 100\n",
    "print(f\" Sale_CC Proportions: {sale_mf_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure Distribution of Clients vs Sale_CC\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(combined_train_data, x='Tenure', hue='Sale_CC',\n",
    "                multiple='stack', bins=30, kde=True)\n",
    "plt.title('Tenure Distribution of Clients vs Sale_CC')\n",
    "plt.xlabel('Tenure (months)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sale_CC', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29408d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count_CC to Sale_CC - Bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=combined_train_data, x='Count_CC', hue='Sale_CC')\n",
    "plt.title('Count_CC Distribution vs Sale_CC')\n",
    "plt.xlabel('Count_CC')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sale_CC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count_CC to Sale_CC - Box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_train_data, x='Sale_CC', y='Count_CC')\n",
    "plt.title('Count_CC Distribution vs Sale_CC')\n",
    "plt.xlabel('Sale_CC')\n",
    "plt.ylabel('Count_CC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Balance vs Sale_CC - Box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_train_data, x='Sale_CC', y='ActBal_CC')\n",
    "plt.title('Actual Balance vs Sale_CC')\n",
    "plt.xlabel('Actual Balance')\n",
    "plt.ylabel('Sale_CC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a61b6",
   "metadata": {},
   "source": [
    "# Predict probabilities of a Sale: "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0e6551c8",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c11d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define feature lists (same as before)\n",
    "numeric_features = [\n",
    "    'Age', 'Tenure',\n",
    "    'Count_CA','Count_SA','Count_CC','Count_OVD','Count_CC','Count_CL',\n",
    "    'ActBal_CA','ActBal_SA','ActBal_CC','ActBal_OVD','ActBal_CC','ActBal_CL',\n",
    "    'VolumeCred','VolumeCred_CA','TransactionsCred','TransactionsCred_CA',\n",
    "    'VolumeDeb','VolumeDeb_CA','VolumeDebCash_Card','VolumeDebCashless_Card',\n",
    "    'VolumeDeb_PaymentOrder','TransactionsDeb','TransactionsDeb_CA',\n",
    "    'TransactionsDebCash_Card','TransactionsDebCashless_Card','TransactionsDeb_PaymentOrder'\n",
    "]\n",
    "categorical_features = ['Sex']\n",
    "\n",
    "# 2. Prepare data\n",
    "X = combined_train_data[numeric_features + categorical_features]\n",
    "y = combined_train_data['Sale_CC']\n",
    "\n",
    "# 3. Create a proper train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Sale_CC positive rate - Train: {y_train.mean():.2%}, Test: {y_test.mean():.2%}\")\n",
    "\n",
    "# 4. Set up the preprocessing and model pipeline (same as before)\n",
    "numeric_transformer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features),\n",
    "])\n",
    "propensity_pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty='l2',\n",
    "        solver='saga',\n",
    "        max_iter=5000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "# 5. Cross-validation for propensity model\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    'auc': make_scorer(roc_auc_score),\n",
    "    'accuracy': 'accuracy'\n",
    "}\n",
    "cv_results = cross_validate(\n",
    "    propensity_pipe, X_train, y_train, \n",
    "    cv=cv, scoring=scoring, \n",
    "    return_train_score=True\n",
    ")\n",
    "# 6. Print cross-validation results\n",
    "print(\"\\nCross-Validation Results (Propensity Model):\")\n",
    "print(f\"Train AUC: {np.mean(cv_results['train_auc']):.4f} \u00b1 {np.std(cv_results['train_auc']):.4f}\")\n",
    "print(f\"CV AUC: {np.mean(cv_results['test_auc']):.4f} \u00b1 {np.std(cv_results['test_auc']):.4f}\")\n",
    "print(f\"Train Accuracy: {np.mean(cv_results['train_accuracy']):.4f} \u00b1 {np.std(cv_results['train_accuracy']):.4f}\")\n",
    "print(f\"CV Accuracy: {np.mean(cv_results['test_accuracy']):.4f} \u00b1 {np.std(cv_results['test_accuracy']):.4f}\")\n",
    "\n",
    "# 7. Final evaluation on test set\n",
    "propensity_pipe.fit(X_train, y_train)\n",
    "test_probas = propensity_pipe.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_probas)\n",
    "print(f\"\\nTest Set AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e67dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "# Make predictions on the test set\n",
    "y_pred = propensity_pipe.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "plt.title('Confusion Matrix for Sale_CC Prediction')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot normalized confusion matrix (percentages)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues', values_format='.2%', ax=ax)\n",
    "plt.title('Normalized Confusion Matrix for Sale_CC Prediction')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Check linearity in the logit\n",
    "def plot_linearity_check(X, y, feature_names, model):\n",
    "    \"\"\"Plot linearity checks for logistic regression\"\"\"\n",
    "    # Get predicted probabilities\n",
    "    probas = model.predict_proba(X)[:, 1]\n",
    "    # Calculate logit (log-odds)\n",
    "    logit = np.log(probas / (1 - probas))\n",
    "    # Create subplots for selected numeric features\n",
    "    n_features = min(20, len(feature_names))  # Limit to 20 plots\n",
    "    fig, axes = plt.subplots(5, 4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(feature_names[:n_features]):\n",
    "        feature_idx = list(X.columns).index(feature) if isinstance(X, pd.DataFrame) else i\n",
    "        feature_values = X.iloc[:, feature_idx] if isinstance(X, pd.DataFrame) else X[:, feature_idx]\n",
    "        \n",
    "        # Plot feature vs logit\n",
    "        axes[i].scatter(feature_values, logit, alpha=0.5)\n",
    "        \n",
    "        # Add lowess smoother to check for linearity\n",
    "        lowess = sm.nonparametric.lowess\n",
    "        z = lowess(logit, feature_values, frac=0.3)\n",
    "        axes[i].plot(z[:, 0], z[:, 1], color='red', linewidth=2)\n",
    "        \n",
    "        axes[i].set_title(f'{feature} vs Log-odds')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Log-odds')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_linearity_check(X, y, numeric_features, propensity_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model performance check\n",
    "def plot_roc_curve(X, y, model):\n",
    "    \"\"\"Plot ROC curve and show AUC\"\"\"\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_proba)\n",
    "    auc = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(X, y, propensity_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ed193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hosmer-Lemeshow test (for goodness of fit)\n",
    "def hosmer_lemeshow_test(y_true, y_pred_proba, n_groups=10):\n",
    "    \"\"\"Perform Hosmer-Lemeshow test\"\"\"\n",
    "    # Create groups based on predicted probabilities\n",
    "    quantiles = np.percentile(y_pred_proba, np.linspace(0, 100, n_groups+1))\n",
    "    quantiles = np.unique(quantiles)\n",
    "    \n",
    "    observed = np.zeros(len(quantiles)-1)\n",
    "    expected = np.zeros(len(quantiles)-1)\n",
    "    total = np.zeros(len(quantiles)-1)\n",
    "    \n",
    "    for i in range(len(quantiles)-1):\n",
    "        if i == len(quantiles)-2:\n",
    "            # Last bin includes upper bound\n",
    "            in_bin = (y_pred_proba >= quantiles[i]) & (y_pred_proba <= quantiles[i+1])\n",
    "        else:\n",
    "            in_bin = (y_pred_proba >= quantiles[i]) & (y_pred_proba < quantiles[i+1])\n",
    "            \n",
    "        observed[i] = np.sum(y_true[in_bin])\n",
    "        expected[i] = np.sum(y_pred_proba[in_bin])\n",
    "        total[i] = np.sum(in_bin)\n",
    "    \n",
    "    # Calculate chi-square statistic\n",
    "    chi_square = np.sum((observed - expected)**2 / (expected * (1 - expected / total)))\n",
    "    df = len(quantiles) - 2\n",
    "    p_value = 1 - stats.chi2.cdf(chi_square, df)\n",
    "    \n",
    "    print(f\"Hosmer-Lemeshow test: chi2={chi_square:.3f}, p-value={p_value:.4f}\")\n",
    "    \n",
    "    # For Hosmer-Lemeshow test, high p-value indicates good fit\n",
    "    if p_value < 0.05:\n",
    "        print(\"The model fit is poor (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"The model fit is adequate (p >= 0.05)\")\n",
    "\n",
    "# Run Hosmer-Lemeshow test\n",
    "y_pred_proba = propensity_pipe.predict_proba(X)[:, 1]\n",
    "hosmer_lemeshow_test(y, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Logistic Regression\n",
    "# Calculate odds ratios\n",
    "def get_logistic_regression_feature_importance(pipe, numeric_features, categorical_features, \n",
    "                                              top_n=20, plot=True, sort_by='abs'):\n",
    "    \"\"\"\n",
    "    Extract feature importance from a logistic regression pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipe : sklearn.pipeline.Pipeline\n",
    "        Trained pipeline containing a logistic regression model\n",
    "    numeric_features : list\n",
    "        List of numeric feature names\n",
    "    categorical_features : list\n",
    "        List of categorical feature names\n",
    "    top_n : int, default=20\n",
    "        Number of top features to display\n",
    "    plot : bool, default=True\n",
    "        Whether to create a visualization of feature importance\n",
    "    sort_by : str, default='abs'\n",
    "        How to sort features: 'abs' (absolute value), 'coef' (raw coefficient), or 'odds' (odds ratio)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with feature importance information\n",
    "    \"\"\"\n",
    "    # Extract logistic regression model from pipeline\n",
    "    logistic_model = pipe.named_steps['clf']\n",
    "    \n",
    "    # Extract feature names after preprocessing\n",
    "    preprocessor = pipe.named_steps['prep']\n",
    "    column_trans_names = []\n",
    "    \n",
    "    # Get numeric feature names directly (they're already the feature names, not indices)\n",
    "    numeric_names = preprocessor.transformers_[0][2]\n",
    "    column_trans_names.extend(numeric_names)\n",
    "    \n",
    "    # Get feature names for categorical features (one-hot encoded)\n",
    "    if len(preprocessor.transformers_) > 1 and categorical_features:\n",
    "        ohe = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "        cat_names = preprocessor.transformers_[1][2]  # These are already the feature names\n",
    "        try:\n",
    "            ohe_feature_names = ohe.get_feature_names_out(cat_names)\n",
    "            column_trans_names.extend(ohe_feature_names)\n",
    "        except:\n",
    "            # Fallback for older sklearn versions\n",
    "            for cat in cat_names:\n",
    "                for category in ohe.categories_[0]:\n",
    "                    column_trans_names.append(f\"{cat}_{category}\")\n",
    "    \n",
    "    # Get coefficients and create DataFrame\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': column_trans_names,\n",
    "        'Coefficient': logistic_model.coef_[0],\n",
    "        'Abs_Coefficient': np.abs(logistic_model.coef_[0])\n",
    "    })\n",
    "    \n",
    "    # Calculate odds ratios\n",
    "    coefficients['Odds_Ratio'] = np.exp(coefficients['Coefficient'])\n",
    "    \n",
    "    # Sort by specified method\n",
    "    if sort_by == 'abs':\n",
    "        coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "    elif sort_by == 'coef':\n",
    "        coefficients = coefficients.sort_values('Coefficient', ascending=False)\n",
    "    elif sort_by == 'odds':\n",
    "        coefficients = coefficients.sort_values('Odds_Ratio', ascending=False)\n",
    "    \n",
    "    # Keep top N features\n",
    "    top_coef = coefficients.head(top_n)\n",
    "    \n",
    "    # Print feature importance\n",
    "    print(f\"Top {top_n} features by importance:\")\n",
    "    display(top_coef[['Feature', 'Coefficient', 'Odds_Ratio']])\n",
    "    \n",
    "    # Plot feature importance\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, top_n/2))\n",
    "        sns.barplot(x='Coefficient', y='Feature', data=top_coef)\n",
    "        plt.title(f'Top {top_n} Features by Coefficient Value')\n",
    "        plt.axvline(x=0, color='gray', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot odds ratios (log scale)\n",
    "        plt.figure(figsize=(12, top_n/2))\n",
    "        sns.barplot(x='Odds_Ratio', y='Feature', data=top_coef)\n",
    "        plt.title(f'Top {top_n} Features by Odds Ratio')\n",
    "        plt.axvline(x=1, color='gray', linestyle='--')\n",
    "        plt.xscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = get_logistic_regression_feature_importance(\n",
    "    propensity_pipe,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    top_n=20,\n",
    "    plot=True,\n",
    "    sort_by='odds'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7659049",
   "metadata": {},
   "source": [
    "# Predict Expected Revenue\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85085808",
   "metadata": {},
   "source": [
    "# Random Forest Regressor "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Revenue model with cross-validation\n",
    "# Filter for only positive cases for revenue model\n",
    "positive_mask_train = y_train == 1\n",
    "X_rev_train = X_train[positive_mask_train]\n",
    "y_rev_train = combined_train_data.loc[X_train.index[positive_mask_train], 'Revenue_CC']\n",
    "\n",
    "revenue_pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('reg', RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=7,\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 9. Cross-validation for revenue model\n",
    "rev_scoring = {\n",
    "    'r2': 'r2',\n",
    "    'mse': make_scorer(mean_squared_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "cv_rev_results = cross_validate(\n",
    "    revenue_pipe, X_rev_train, y_rev_train, \n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "    scoring=rev_scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results (Revenue Model):\")\n",
    "print(f\"Train R\u00b2: {np.mean(cv_rev_results['train_r2']):.4f} \u00b1 {np.std(cv_rev_results['train_r2']):.4f}\")\n",
    "print(f\"CV R\u00b2: {np.mean(cv_rev_results['test_r2']):.4f} \u00b1 {np.std(cv_rev_results['test_r2']):.4f}\")\n",
    "print(f\"Train MSE: {-np.mean(cv_rev_results['train_mse']):.2f} \u00b1 {np.std(cv_rev_results['train_mse']):.2f}\")\n",
    "print(f\"CV MSE: {-np.mean(cv_rev_results['test_mse']):.2f} \u00b1 {np.std(cv_rev_results['test_mse']):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 10. Final revenue model evaluation\n",
    "positive_mask_test = y_test == 1\n",
    "X_rev_test = X_test[positive_mask_test]\n",
    "y_rev_test = combined_train_data.loc[X_test.index[positive_mask_test], 'Revenue_CC']\n",
    "\n",
    "revenue_pipe.fit(X_rev_train, y_rev_train)\n",
    "y_rev_pred = revenue_pipe.predict(X_rev_test)\n",
    "\n",
    "# Calculate adjusted R-squared and other model evaluation metrics\n",
    "def evaluate_regression_model(y_true, y_pred, X, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of regression model with adjusted R-squared\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual target values\n",
    "    y_pred : array-like\n",
    "        Predicted target values\n",
    "    X : DataFrame or array-like\n",
    "        Feature matrix (needed for adjusted R-squared calculation)\n",
    "    model_name : str\n",
    "        Name of the model for display purposes\n",
    "    \"\"\"\n",
    "    n = len(y_true)  # Sample size\n",
    "    p = X.shape[1]   # Number of predictors\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Number of features: {p}\")\n",
    "    print(f\"R\u00b2: {r2:.4f}\")\n",
    "    print(f\"Adjusted R\u00b2: {adj_r2:.4f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'adj_r2': adj_r2,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# Evaluate the revenue model\n",
    "revenue_metrics = evaluate_regression_model(y_rev_test, y_rev_pred, X_rev_test, model_name=\"Revenue Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted Revenue Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_rev_test, y_rev_pred, alpha=0.5)\n",
    "plt.plot([y_rev_test.min(), y_rev_test.max()], [y_rev_test.min(), y_rev_test.max()], 'r--')\n",
    "plt.xlabel('Actual Revenue')\n",
    "plt.ylabel('Predicted Revenue')\n",
    "plt.title('Random Forest: Actual vs Predicted Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Revenue Model\n",
    "\n",
    "def get_random_forest_feature_importance(pipe, numeric_features, categorical_features, \n",
    "        top_n=20, plot=True):\n",
    "    \"\"\"\n",
    "    Extract feature importance from a random forest pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipe : sklearn.pipeline.Pipeline\n",
    "        Trained pipeline containing a random forest model\n",
    "    numeric_features : list\n",
    "        List of numeric feature names\n",
    "    categorical_features : list\n",
    "        List of categorical feature names\n",
    "    top_n : int, default=20\n",
    "        Number of top features to display\n",
    "    plot : bool, default=True\n",
    "        Whether to create a visualization of feature importance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with feature importance information\n",
    "    \"\"\"\n",
    "    # Extract random forest model from pipeline\n",
    "    rf_model = pipe.named_steps['reg']\n",
    "    \n",
    "    # Extract feature names after preprocessing\n",
    "    preprocessor = pipe.named_steps['prep']\n",
    "    column_trans_names = []\n",
    "    \n",
    "    # Get numeric feature names directly\n",
    "    numeric_names = preprocessor.transformers_[0][2]\n",
    "    column_trans_names.extend(numeric_names)\n",
    "    \n",
    "    # Get feature names for categorical features (one-hot encoded)\n",
    "    if len(preprocessor.transformers_) > 1 and categorical_features:\n",
    "        ohe = preprocessor.transformers_[1][1].named_steps['onehot']\n",
    "        cat_names = preprocessor.transformers_[1][2]\n",
    "        try:\n",
    "            ohe_feature_names = ohe.get_feature_names_out(cat_names)\n",
    "            column_trans_names.extend(ohe_feature_names)\n",
    "        except:\n",
    "            # Fallback for older sklearn versions\n",
    "            for cat in cat_names:\n",
    "                for category in ohe.categories_[0]:\n",
    "                    column_trans_names.append(f\"{cat}_{category}\")\n",
    "    \n",
    "    # Get feature importances and create DataFrame\n",
    "    importances = pd.DataFrame({\n",
    "        'Feature': column_trans_names,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importances = importances.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Keep top N features\n",
    "    top_importances = importances.head(top_n)\n",
    "    \n",
    "    # Print feature importance\n",
    "    print(f\"Top {top_n} features by importance:\")\n",
    "    display(top_importances)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, top_n/2))\n",
    "        sns.barplot(x='Importance', y='Feature', data=top_importances)\n",
    "        plt.title(f'Top {top_n} Features by Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importances\n",
    "\n",
    "revenue_importances = get_random_forest_feature_importance(\n",
    "    revenue_pipe,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    top_n=20,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "import numpy as np\n",
    "# Note: LinearSVC is removed as it is a classifier\n",
    "from sklearn.linear_model import RidgeCV, SGDRegressor, ElasticNetCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Assuming X_rev_train, y_rev_train, X_rev_test, and y_rev_test are already defined pandas DataFrames or Series\n",
    "\n",
    "# All models in this list are now regressors\n",
    "trylist = [\n",
    "    RidgeCV,\n",
    "    SGDRegressor,\n",
    "    ElasticNetCV,\n",
    "    LassoCV,\n",
    "    KNeighborsRegressor\n",
    "]\n",
    "\n",
    "# Create a smaller sample of the data\n",
    "sample_size = min(50, len(X_rev_train))\n",
    "sample_indices = np.random.choice(len(X_rev_train), sample_size, replace=False)\n",
    "\n",
    "X_train_sample = X_rev_train.iloc[sample_indices].reset_index(drop=True)\n",
    "y_train_sample = y_rev_train.iloc[sample_indices].reset_index(drop=True)\n",
    "\n",
    "# Create a smaller test set\n",
    "test_sample_size = min(20, len(X_rev_test))\n",
    "test_indices = np.random.choice(len(X_rev_test), test_sample_size, replace=False)\n",
    "X_test_sample = X_rev_test.iloc[test_indices].reset_index(drop=True)\n",
    "y_test_sample = y_rev_test.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "# Initialize LazyRegressor with the corrected list of regressors\n",
    "lazy_reg = LazyRegressor(\n",
    "    custom_metric=None,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    regressors=trylist\n",
    ")\n",
    "\n",
    "print(f\"Using sample sizes - Train: {len(X_train_sample)}, Test: {len(X_test_sample)}\")\n",
    "\n",
    "# Fit LazyRegressor on the smaller sampled data\n",
    "models, predictions = lazy_reg.fit(X_train_sample, X_test_sample, y_train_sample, y_test_sample)\n",
    "\n",
    "# Display the results\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a92df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
